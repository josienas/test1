import mlflow
import mlflow.sklearn
import time
import math
import requests
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# è¨­å®š MLflow tracking URI
mlflow.set_tracking_uri("http://localhost:5000")

st.set_page_config(page_title="BTC Direction Predictor", layout="wide")
st.title("BTC/USDT æ¼²è·Œæ–¹å‘é æ¸¬")

#666666
# ---------------------------
# Binance download
# ---------------------------
def fetch_binance_klines(symbol="BTCUSDT", interval="1d", limit=1000, end_time_ms=None):
    url = "https://api.binance.com/api/v3/klines"
    params = {"symbol": symbol, "interval": interval, "limit": limit}
    if end_time_ms is not None:
        params["endTime"] = int(end_time_ms)
    r = requests.get(url, params=params, timeout=20)
    r.raise_for_status()
    return r.json()

@st.cache_data(show_spinner=False)
def download_history(symbol="BTCUSDT", interval="1d", total_points=3000, sleep_sec=0.15):
    all_rows = []
    end_time = None
    while len(all_rows) < total_points:
        data = fetch_binance_klines(symbol=symbol, interval=interval, limit=1000, end_time_ms=end_time)
        if not data:
            break
        all_rows = data + all_rows
        oldest_open_time = data[0][0]
        end_time = oldest_open_time - 1
        time.sleep(sleep_sec)
        if len(data) < 1000:
            break

    all_rows = all_rows[-total_points:]

    cols = [
        "open_time", "open", "high", "low", "close", "volume",
        "close_time", "quote_asset_volume", "num_trades",
        "taker_buy_base_vol", "taker_buy_quote_vol", "ignore"
    ]
    df = pd.DataFrame(all_rows, columns=cols)
    df["date"] = pd.to_datetime(df["open_time"], unit="ms")
    for c in ["open", "high", "low", "close", "volume"]:
        df[c] = df[c].astype(float)
    df = df[["date", "open", "high", "low", "close", "volume"]]
    df = df.drop_duplicates(subset="date")
    df = df.sort_values("date").reset_index(drop=True)
    return df

@st.cache_data(show_spinner=False)
def load_csv_by_interval(interval: str, total_points: int):
    """
    Read local CSV file:
      btc_1h.csv / btc_2h.csv / btc_4h.csv / btc_1d.csv
    Accepts either:
      - columns: open_time, open, high, low, close, volume
      - or columns: date, open, high, low, close, volume
    """
    path = f"btc_{interval}.csv"
    df = pd.read_csv(path)

    # normalize datetime column
    if "open_time" in df.columns:
        df["date"] = pd.to_datetime(df["open_time"])
    elif "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"])
    else:
        raise ValueError(f"{path} ç¼ºå°‘ open_time æˆ– date æ¬„ä½")

    # normalize numeric
    for c in ["open", "high", "low", "close", "volume"]:
        df[c] = df[c].astype(float)

    df = df[["date", "open", "high", "low", "close", "volume"]]
    df = df.drop_duplicates(subset="date")
    df = df.sort_values("date").reset_index(drop=True)

    if total_points < len(df):
        df = df.tail(total_points).reset_index(drop=True)

    return df


# ---------------------------
# Feature + label
# ---------------------------
def make_features_and_label(df: pd.DataFrame, horizon_bars: int, ma_short=5, ma_long=10, vol_w=5) -> pd.DataFrame:
    out = df.copy()

    # ---------- Basic returns ----------
    out["return_1"] = out["close"].pct_change(1)
    out["return_3"] = out["close"].pct_change(3)
    out["return_7"] = out["close"].pct_change(7)

    # ---------- Price range / candle structure ----------
    out["hl_range"] = (out["high"] - out["low"]) / out["close"]                 # (H-L)/C
    out["oc_return"] = (out["close"] - out["open"]) / out["open"]               # (C-O)/O
    out["upper_wick"] = (out["high"] - out[["open", "close"]].max(axis=1)) / out["close"]
    out["lower_wick"] = (out[["open", "close"]].min(axis=1) - out["low"]) / out["close"]

    # ---------- Volume features ----------
    out["vol_chg_1"] = out["volume"].pct_change(1)
    out["vol_ma_20"] = out["volume"].rolling(20).mean()
    out["vol_ratio_20"] = out["volume"] / out["vol_ma_20"]

    # ---------- Moving averages + bias ----------
    out["ma_s"] = out["close"].rolling(ma_short).mean()
    out["ma_l"] = out["close"].rolling(ma_long).mean()
    out["ma_bias_s"] = (out["close"] - out["ma_s"]) / out["ma_s"]
    out["ma_bias_l"] = (out["close"] - out["ma_l"]) / out["ma_l"]

    # ---------- Volatility ----------
    out["vol"] = out["return_1"].rolling(vol_w).std()

    # ---------- RSI(14) ----------
    delta = out["close"].diff()
    gain = delta.clip(lower=0)
    loss = (-delta).clip(lower=0)
    avg_gain = gain.rolling(14).mean()
    avg_loss = loss.rolling(14).mean()
    rs = avg_gain / (avg_loss + 1e-12)
    out["rsi_14"] = 100 - (100 / (1 + rs))

    # ---------- MACD(12,26,9) ----------
    ema12 = out["close"].ewm(span=12, adjust=False).mean()
    ema26 = out["close"].ewm(span=26, adjust=False).mean()
    out["macd"] = ema12 - ema26
    out["macd_signal"] = out["macd"].ewm(span=9, adjust=False).mean()
    out["macd_hist"] = out["macd"] - out["macd_signal"]

    # ---------- Bollinger Bands(20,2) ----------
    bb_mid = out["close"].rolling(20).mean()
    bb_std = out["close"].rolling(20).std()
    out["bb_mid"] = bb_mid
    out["bb_upper"] = bb_mid + 2 * bb_std
    out["bb_lower"] = bb_mid - 2 * bb_std
    out["bb_width"] = (out["bb_upper"] - out["bb_lower"]) / (out["bb_mid"] + 1e-12)
    out["bb_pos"] = (out["close"] - out["bb_lower"]) / ((out["bb_upper"] - out["bb_lower"]) + 1e-12)

    # ---------- ATR(14) ----------
    prev_close = out["close"].shift(1)
    tr = pd.concat([
        (out["high"] - out["low"]),
        (out["high"] - prev_close).abs(),
        (out["low"] - prev_close).abs()
    ], axis=1).max(axis=1)
    out["atr_14"] = tr.rolling(14).mean()
    out["atr_pct_14"] = out["atr_14"] / (out["close"] + 1e-12)

    # ---------- Label: N bars later close > now close ----------
    out["future_close"] = out["close"].shift(-horizon_bars)
    out["y"] = (out["future_close"] > out["close"]).astype(int)

    return out.dropna().reset_index(drop=True)


def interval_to_hours(interval: str) -> int:
    mapping = {"1h": 1, "2h": 2, "4h": 4, "1d": 24}
    return mapping[interval]

def _ensure_enough_rows(df: pd.DataFrame, horizon_bars: int, ma_long: int, vol_w: int) -> int:
    """Return minimum required rows; raise Streamlit stop if insufficient."""
    min_lookback = max(ma_long, vol_w, 26, 20, 14)  # longest window used in features
    min_rows = min_lookback + horizon_bars + 2  # buffer to allow dropna and splitting
    if len(df) < min_rows:
        st.error(
            f"è³‡æ–™ä¸è¶³ï¼ˆç›®å‰ {len(df)} ç­†ï¼Œè‡³å°‘éœ€è¦ {min_rows} ç­†ï¼‰æ‰èƒ½è¨ˆç®—ç‰¹å¾µèˆ‡åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†ï¼Œ"
            f"è«‹æ¸›å°‘ horizon æˆ–å¢åŠ è³‡æ–™ç­†æ•¸ã€‚"
        )
        st.stop()
    return min_rows


def _safe_train_test_split(X: pd.DataFrame, y: pd.Series, train_ratio: float):
    split_idx = int(len(X) * train_ratio)
    split_idx = min(max(split_idx, 1), len(X) - 1)
    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
    if X_train.empty or X_test.empty:
        st.error("è³‡æ–™åˆ†å‰²å¾Œè¨“ç·´/æ¸¬è©¦é›†æœ‰ç©ºå€¼ï¼Œè«‹èª¿æ•´ train ratio æˆ–è³‡æ–™ç­†æ•¸ã€‚")
        st.stop()
    return X_train, X_test, y_train, y_test, split_idx


# ---------------------------
# UI Controls
# ---------------------------
st.sidebar.header("è¨­å®š")

data_source = st.sidebar.radio("è³‡æ–™ä¾†æº", ["æŠ“ Binanceï¼ˆå³æ™‚ï¼‰", "è®€æœ¬æ©Ÿ CSVï¼ˆbtc_1h.csv...ï¼‰"], index=0)

interval = st.sidebar.selectbox("K ç·šæ™‚é–“å°ºåº¦", ["1h", "2h", "4h", "1d"], index=3)  # default 1d
total_points = st.sidebar.slider("è³‡æ–™ç­†æ•¸ï¼ˆè¶Šå¤§è¶Šä¹…ï¼‰", 500, 5000, 3000, 500)

horizon_hours = st.sidebar.selectbox("é æ¸¬å¤šä¹…å¾Œï¼ˆå°æ™‚ï¼‰", [1, 2, 4, 8, 12, 24], index=1)

train_ratio = st.sidebar.slider("Train ratio", 0.5, 0.9, 0.7, 0.05)
n_estimators = st.sidebar.slider("n_estimators", 50, 500, 200, 50)
max_depth = st.sidebar.slider("max_depth", 2, 20, 5, 1)

seed = st.sidebar.number_input("random_stateï¼ˆseedï¼‰", min_value=0, max_value=9999, value=42, step=1)

st.sidebar.markdown("---")
# MA sliders (hidden)
# ma_short = st.sidebar.slider("MA shortï¼ˆbarsï¼‰", 3, 60, 5, 1)
# ma_long = st.sidebar.slider("MA longï¼ˆbarsï¼‰", 5, 120, 10, 1)
# vol_w = st.sidebar.slider("Vol windowï¼ˆbarsï¼‰", 3, 60, 5, 1)
ma_short = 5
ma_long = 10
vol_w = 5

# âœ… å¼·åˆ¶åˆ·æ–°æŒ‰éˆ•ï¼ˆæ¸… cache + rerunï¼‰
if st.sidebar.button("ğŸ”„ å¼·åˆ¶é‡æ–°æŠ“è³‡æ–™ / é‡è¨“"):
    st.cache_data.clear()
    st.rerun()

# horizon bars (use ceil)
bar_hours = interval_to_hours(interval)
horizon_bars = max(1, math.ceil(horizon_hours / bar_hours))

st.info(
    f"ä½ é¸çš„æ˜¯ï¼šinterval = {interval}ï¼ˆæ¯æ ¹ {bar_hours} å°æ™‚ï¼‰ï½œ"
    f"é æ¸¬ = {horizon_hours} å°æ™‚å¾Œ â‰ˆ {horizon_bars} æ ¹K å¾Œ"
)


# ---------------------------
# Load data (Binance or CSV)
# ---------------------------
if data_source.startswith("æŠ“ Binance"):
    with st.spinner("æŠ“å– Binance è³‡æ–™ä¸­..."):
        df_raw = download_history(symbol="BTCUSDT", interval=interval, total_points=total_points)
else:
    with st.spinner("è®€å–æœ¬æ©Ÿ CSV ä¸­..."):
        # éœ€è¦ä½ è³‡æ–™å¤¾å…§æœ‰ btc_1h.csv / btc_2h.csv / btc_4h.csv / btc_1d.csv
        df_raw = load_csv_by_interval(interval=interval, total_points=total_points)

df_raw = df_raw.copy()
df_raw = df_raw.dropna(subset=["date", "close"])


# âœ… é©—è­‰ï¼šä½ çœŸçš„æ›åˆ° interval äº†å—ï¼Ÿ
st.subheader("è³‡æ–™æª¢æŸ¥")
c1, c2, c3 = st.columns(3)
c1.write("è³‡æ–™ç­†æ•¸")
c1.metric("rows", f"{len(df_raw)}")

c2.write("æ™‚é–“ç¯„åœ")
c2.write(f"{df_raw['date'].min()}  ~  {df_raw['date'].max()}")

c3.write("æœ€å¾Œå…©ç­†æ™‚é–“å·®")
if len(df_raw) >= 2:
    c3.write(df_raw["date"].iloc[-1] - df_raw["date"].iloc[-2])
else:
    c3.write("è³‡æ–™ä¸è¶³")

st.subheader("åŸå§‹è³‡æ–™ï¼ˆæœ€å¾Œ 10 ç­†ï¼‰")
st.dataframe(df_raw.tail(10), use_container_width=True)


# ---------------------------
# Build dataset
# ---------------------------
_ensure_enough_rows(df_raw, horizon_bars=horizon_bars, ma_long=ma_long, vol_w=vol_w)

df = make_features_and_label(df_raw, horizon_bars=horizon_bars, ma_short=ma_short, ma_long=ma_long, vol_w=vol_w)

if df.empty:
    st.error("ç‰¹å¾µè¨ˆç®—å¾Œæ²’æœ‰å¯ç”¨è³‡æ–™ï¼Œè«‹èª¿æ•´åƒæ•¸æˆ–å¢åŠ è³‡æ–™ç­†æ•¸ã€‚")
    st.stop()


feature_cols = [
    # returns
    "return_1", "return_3", "return_7",

    # candle / range
    "hl_range", "oc_return", "upper_wick", "lower_wick",

    # volume
    "vol_chg_1", "vol_ratio_20",

    # MA + bias
    "ma_s", "ma_l", "ma_bias_s", "ma_bias_l",

    # volatility
    "vol",

    # RSI / MACD
    "rsi_14", "macd", "macd_signal", "macd_hist",

    # Bollinger / ATR
    "bb_width", "bb_pos", "atr_pct_14",
]

X = df[feature_cols]
y = df["y"]

X_train, X_test, y_train, y_test, split_idx = _safe_train_test_split(X, y, train_ratio)

# ---------------------------
# Train + predict
# ---------------------------
# ç¢ºä¿çµæŸä»»ä½•å‰ä¸€å€‹ run
mlflow.end_run()

with mlflow.start_run(nested=False):
    # è¨˜éŒ„åƒæ•¸
    mlflow.log_param("n_estimators", n_estimators)
    mlflow.log_param("max_depth", max_depth)
    mlflow.log_param("train_ratio", train_ratio)
    mlflow.log_param("horizon_hours", horizon_hours)
    mlflow.log_param("interval", interval)

    # è¨“ç·´
    rf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=seed
    )
    rf.fit(X_train, y_train)
    y_pred = rf.predict(X_test)

    # è©•ä¼°
    acc = accuracy_score(y_test, y_pred)
    mlflow.log_metric("accuracy", acc)

    # å­˜æ¨¡å‹
    mlflow.sklearn.log_model(rf, "random_forest_model")
    
baseline = max((y_test == 1).mean(), (y_test == 0).mean())

col1, col2, col3, col4, col5 = st.columns(5)

col1.metric("Test Accuracy", f"{acc:.6f}")
col2.metric("Majority baseline", f"{baseline:.6f}")
col3.metric("Train size", f"{len(X_train)}")
col4.metric("Test size", f"{len(X_test)}")
col5.metric("Pred Up ratio", f"{(y_pred == 1).mean():.3f}")


# æœ€æ–°é æ¸¬
latest_features_time = df.iloc[-1]["date"]
latest_pred = rf.predict(X.iloc[[-1]])[0]
latest_label = "Upï¼ˆé æ¸¬æœƒæ¼²ï¼‰" if latest_pred == 1 else "Downï¼ˆé æ¸¬æœƒè·Œ/ä¸æ¼²ï¼‰"
st.success(f"ğŸ“Œ æœ€æ–°ç‰¹å¾µæ™‚é–“ï¼ˆ{latest_features_time}ï¼‰â†’ æ¨¡å‹é æ¸¬ **{horizon_hours} å°æ™‚å¾Œ**ï¼š**{latest_label}**")


# ---------------------------
# Eval (hidden)
# ---------------------------
# st.subheader("æ··æ·†çŸ©é™£ / å ±å‘Š")
# cm = confusion_matrix(y_test, y_pred)
# st.write("Confusion Matrix [[TN FP],[FN TP]]:")
# st.write(cm)
# st.text(classification_report(y_test, y_pred, digits=4))


# ---------------------------
# Feature importance plot (hidden)
# ---------------------------
# st.subheader("ç‰¹å¾µé‡è¦æ€§")
# importances = pd.Series(rf.feature_importances_, index=feature_cols).sort_values(ascending=False)
# fig1 = plt.figure()
# importances.plot(kind="bar")
# plt.title("Feature Importance")
# plt.tight_layout()
# st.pyplot(fig1)


# ---------------------------
# True vs Pred plot (hidden)
# ---------------------------
# st.subheader("True vs Predï¼ˆæ¸¬è©¦é›†æ–¹å‘ï¼‰")
# plot_df = df.iloc[split_idx:].copy()
# plot_df["y_true"] = y_test.values
# plot_df["y_pred"] = y_pred
# 
# fig2 = plt.figure(figsize=(12, 3.5))
# plt.plot(plot_df["date"], plot_df["y_true"], label="True", alpha=0.75)
# plt.plot(plot_df["date"], plot_df["y_pred"], label="Pred", alpha=0.75)
# plt.yticks([0, 1], ["Down", "Up"])
# plt.title("True vs Pred (Test Set)")
# plt.legend()
# plt.tight_layout()
# st.pyplot(fig2)


# ---------------------------
# Show a few predictions table
# ---------------------------
st.subheader("æ¸¬è©¦é›†å‰ 15 ç­†ï¼šçœŸå¯¦ vs é æ¸¬")
# å› ç‚º plot_df è¢«å®šç¾©åœ¨è¢«è¨»è§£çš„å€æ®µä¸­ï¼Œéœ€è¦åœ¨é€™è£¡é‡æ–°å®šç¾©
plot_df = df.iloc[split_idx:].copy()
plot_df["y_true"] = y_test.values
plot_df["y_pred"] = y_pred

show_df = plot_df[["date", "close", "y_true", "y_pred"]].head(15).copy()
show_df["y_true_label"] = show_df["y_true"].map({1: "Up", 0: "Down"})
show_df["y_pred_label"] = show_df["y_pred"].map({1: "Up", 0: "Down"})
st.dataframe(show_df, use_container_width=True)

